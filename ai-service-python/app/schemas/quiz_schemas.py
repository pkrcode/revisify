from pydantic import BaseModel
from typing import List, Literal

# --- 1. Schemas for Quiz GENERATION ---

class QuizGenerationRequest(BaseModel):
    """
    Defines the request sent from the Node.js backend to generate a quiz.
    It specifies the documents to use and the number of each question type.
    """
    pdfIds: List[str]
    numMCQs: int
    numSAQs: int
    numLAQs: int

# Schemas defining the structure of the questions generated by the AI.
# The `ideal_answer` or `correct_answer` is generated by the AI and used for grading.
# It is not initially shown to the user.

class MCQ(BaseModel):
    question_type: Literal['mcq'] = 'mcq'
    question: str
    options: List[str]
    correct_answer: str

class SAQ(BaseModel):
    question_type: Literal['saq'] = 'saq'
    question: str
    ideal_answer: str

class LAQ(BaseModel):
    question_type: Literal['laq'] = 'laq'
    question: str
    ideal_answer: str

class GeneratedQuiz(BaseModel):
    """
    Defines the full quiz structure that the AI service will generate.
    """
    mcqs: List[MCQ]
    saqs: List[SAQ]
    laqs: List[LAQ]


# --- 2. Schemas for Quiz GRADING ---

class QuestionToGrade(BaseModel):
    """
    Defines the structure for a single question that the Node.js backend
    sends to the AI service for grading.
    """
    question: str
    user_answer: str
    ideal_answer: str
    question_type: Literal['mcq', 'saq', 'laq']

class QuizGradingRequest(BaseModel):
    """
    Defines the full request payload for grading a quiz submission.
    """
    questions_to_grade: List[QuestionToGrade]

# Schemas defining the structure of the grading response from the AI.

class GradedQuestion(BaseModel):
    """
    Defines the result for a single graded question.
    """
    question: str
    score: int
    explanation: str # AI-generated feedback on why the user got the score.

class QuizGradingResponse(BaseModel):
    """
    Defines the full response from the grading service.
    """
    graded_questions: List[GradedQuestion]
    total_score: int
